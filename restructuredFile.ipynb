{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Mining Group 4 Jupyter Notebook**\n",
    "\n",
    "By: Assaf Bohen, Adrian Nica, TJ Jablonski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "def download_and_extract_csv(zip_url):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from `zip_url`, extracts the first .csv file found,\n",
    "    and returns it as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    #determines if zip is found\n",
    "    response = requests.get(zip_url)\n",
    "    if response.status_code == 200:\n",
    "        zip_content = io.BytesIO(response.content)\n",
    "        \n",
    "        #extract csv files\n",
    "        with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "            for file_name in zip_ref.namelist():\n",
    "                if file_name.endswith('.csv'):\n",
    "                    with zip_ref.open(file_name) as csv_file:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "\n",
    "                        #print success\n",
    "                        parsed_url = urlparse(zip_url)\n",
    "                        print(f'Successfully extracted {file_name} from {os.path.basename(parsed_url.path)}')\n",
    "                        return df\n",
    "\n",
    "        #no csv files found\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve ZIP from {zip_url}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls to zip files\n",
    "csv_url_2024 = \"https://cdn.sanity.io/files/jo7n4k8s/production/262f04c41d99fea692e0125c342e446782233fe4.zip/stack-overflow-developer-survey-2024.zip\"\n",
    "csv_url_2023 = \"https://cdn.stackoverflow.co/files/jo7n4k8s/production/49915bfd46d0902c3564fd9a06b509d08a20488c.zip/stack-overflow-developer-survey-2023.zip\"\n",
    "csv_url_2022 = \"https://info.stackoverflowsolutions.com/rs/719-EMH-566/images/stack-overflow-developer-survey-2022.zip\"\n",
    "\n",
    "#download csv files\n",
    "odf_2024 = download_and_extract_csv(csv_url_2024)\n",
    "odf_2023 = download_and_extract_csv(csv_url_2023)\n",
    "odf_2022 = download_and_extract_csv(csv_url_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts column names to uppercase and removes spacing\n",
    "odf_2024.columns = odf_2024.columns.str.upper().str.replace(\" \", \"\")\n",
    "odf_2023.columns = odf_2023.columns.str.upper().str.replace(\" \", \"\")\n",
    "odf_2022.columns = odf_2022.columns.str.upper().str.replace(\" \", \"\")\n",
    "\n",
    "#prints all rows\n",
    "with pd.option_context('display.max_columns', None,\n",
    "                       'display.max_rows', None,\n",
    "                       'display.width', 6000):\n",
    "    print(odf_2024.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all cols from odf_2024 that didn't select Apples (only survey to check for bots)\n",
    "print(f'odf_2024 rows before dropping bots: {odf_2024.shape[0]}')\n",
    "odf_2024 = odf_2024[odf_2024['CHECK'] == 'Apples']\n",
    "print(f'odf_2024 rows after dropping bots: {odf_2024.shape[0]}\\n')\n",
    "\n",
    "#finds common column names across all dfs\n",
    "common_cols = set(odf_2024.columns).intersection(odf_2023.columns).intersection(odf_2022.columns)\n",
    "\n",
    "#creates copies of dfs\n",
    "df_2024 = odf_2024[list(common_cols)].copy()\n",
    "df_2023 = odf_2023[list(common_cols)].copy()\n",
    "df_2022 = odf_2022[list(common_cols)].copy()\n",
    "\n",
    "#adds a year column to all dfs\n",
    "df_2024[\"year\"] = 2024\n",
    "df_2023[\"year\"] = 2023\n",
    "df_2022[\"year\"] = 2022\n",
    "\n",
    "#combined the common columns of the dfs\n",
    "combined_df = pd.concat([df_2024, df_2023, df_2022], ignore_index=True)\n",
    "\n",
    "#prints shape and all rows\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "with pd.option_context('display.max_columns', None,\n",
    "                       'display.max_rows', None,\n",
    "                       'display.width', 6000):\n",
    "    print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------                       \n",
    ".\n",
    "\n",
    "**Compensation Data Exploration**\n",
    "\n",
    ".\n",
    "\n",
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show changes\n",
    "print(f'Number of columns in compensation_df: {combined_df.shape[1]}')\n",
    "\n",
    "#columns to include in compensatiion_df\n",
    "compensation_include_columns = [\n",
    "    'CONVERTEDCOMPYEARLY',\n",
    "    'COUNTRY',\n",
    "    'DEVTYPE',\n",
    "    'EDLEVEL',\n",
    "    'YEARSCODE',\n",
    "    'YEARSCODEPRO',\n",
    "    'ORGSIZE'\n",
    "]\n",
    "\n",
    "#creates jobsat_df\n",
    "compensation_df = combined_df[compensation_include_columns]\n",
    "\n",
    "#show changes\n",
    "print(f'Number of columns in compensation_df: {compensation_df.shape[1]}')\n",
    "compensation_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all NaN values in CONVERTEDCOMPYEARLY column\n",
    "print(f'Rows before dropping NaN CCY values: {compensation_df.shape[0]}')\n",
    "compensation_df = compensation_df.dropna(subset=['CONVERTEDCOMPYEARLY'])\n",
    "print(f'Rows after dropping NaN CCY values: {compensation_df.shape[0]}\\n')\n",
    "\n",
    "#drop all remaining NaN values\n",
    "print(f'Rows before dropping all remaining NaN values: {compensation_df.shape[0]}')\n",
    "compensation_df = compensation_df.dropna()\n",
    "print(f'Rows after dropping all remaining NaN values: {compensation_df.shape[0]}\\n')\n",
    "\n",
    "#ensure no more NaN\n",
    "if sum(compensation_df.isna().sum()) == 0:\n",
    "    print('All NaN values successfully removed\\n')\n",
    "else:\n",
    "    print('Dataset still contains NaN values\\n')\n",
    "\n",
    "#drop countries that appear less than 10 times\n",
    "country_counts = compensation_df['COUNTRY'].value_counts()\n",
    "rare_countries = country_counts[country_counts > 10].index\n",
    "print(f\"Rows before dropping countries that don't appear at least 10 times: {compensation_df.shape[0]}\")\n",
    "compensation_df = compensation_df[compensation_df['COUNTRY'].isin(rare_countries)]\n",
    "print(f\"Rows after dropping countries that don't appear at least 10 times: {compensation_df.shape[0]}\")\n",
    "\n",
    "#drop outliers based on 1.5*IQR\n",
    "print(f\"Rows before dropping compensation outliers: {compensation_df.shape[0]}\")\n",
    "Q1 = compensation_df['CONVERTEDCOMPYEARLY'].quantile(0.25)\n",
    "Q3 = compensation_df['CONVERTEDCOMPYEARLY'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "compensation_df = compensation_df[\n",
    "    (compensation_df['CONVERTEDCOMPYEARLY'] >= Q1 - 1.5 * IQR) &\n",
    "    (compensation_df['CONVERTEDCOMPYEARLY'] <= Q3 + 1.5 * IQR)\n",
    "]\n",
    "print(f\"Rows after dropping compensation outliers: {compensation_df.shape[0]}\")\n",
    "\n",
    "#reset index\n",
    "compensation_df = compensation_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#manually ordinal encode\n",
    "orgsize_order = {\n",
    "    'Just me - I am a freelancer, sole proprietor, etc.' : 1,\n",
    "    '2 to 9 employees' : 2,\n",
    "    '10 to 19 employees' : 3,\n",
    "    '20 to 99 employees' : 4,\n",
    "    '100 to 499 employees' : 5,\n",
    "    '500 to 999 employees' : 6,\n",
    "    '1,000 to 4,999 employees' : 7,\n",
    "    '5,000 to 9,999 employees' : 8,\n",
    "    '10,000 or more employees' : 9,\n",
    "    \"I donâ€™t know\" : -1\n",
    "}\n",
    "compensation_df['ORGSIZE'] = compensation_df['ORGSIZE'].map(orgsize_order)\n",
    "\n",
    "#hot encode COUNTRY and add to compensation_df\n",
    "country_dummies = pd.get_dummies(compensation_df['COUNTRY'], prefix='COUNTRY').astype(int)\n",
    "compensation_df = pd.concat([compensation_df.drop(columns=['COUNTRY']), country_dummies], axis=1)\n",
    "\n",
    "#hot encode EDLEVEL and add to compensation_df\n",
    "edlevel_dummies = pd.get_dummies(compensation_df['EDLEVEL'], prefix='EDLEVEL').astype(int)\n",
    "compensation_df = pd.concat([compensation_df.drop(columns=['EDLEVEL']), edlevel_dummies], axis=1)\n",
    "\n",
    "#ultilabel encode DEVTYPE\n",
    "compensation_df['DEVTYPE'] = compensation_df['DEVTYPE'].str.split(';').apply(lambda x: [i.strip() for i in x] if isinstance(x, list) else [])\n",
    "mlb = MultiLabelBinarizer()\n",
    "devtype_encoded = mlb.fit_transform(compensation_df['DEVTYPE'])\n",
    "devtype_df = pd.DataFrame(devtype_encoded, columns=mlb.classes_, index=compensation_df.index)\n",
    "compensation_df = pd.concat([compensation_df.drop(columns=['DEVTYPE']), devtype_df], axis=1)\n",
    "\n",
    "#convert YEARSCODE to int\n",
    "compensation_df['YEARSCODE'] = compensation_df['YEARSCODE'].replace({\n",
    "    'More than 50 years': '50',\n",
    "    'Less than 1 year': '1'\n",
    "})\n",
    "compensation_df['YEARSCODE'] = compensation_df['YEARSCODE'].astype(int)\n",
    "\n",
    "#convert YEARSCODEPRO to int\n",
    "compensation_df['YEARSCODEPRO'] = compensation_df['YEARSCODEPRO'].replace({\n",
    "    'More than 50 years': '50',\n",
    "    'Less than 1 year': '1'\n",
    "})\n",
    "compensation_df['YEARSCODEPRO'] = compensation_df['YEARSCODEPRO'].astype(int)\n",
    "\n",
    "#rename columns and ensures all numeric\n",
    "numeric = True\n",
    "compensation_df = compensation_df.rename(columns={'CONVERTEDCOMPYEARLY': 'COMPENSATION'})\n",
    "for col in compensation_df.columns:\n",
    "    if compensation_df[col].dtype not in [int, float, 'int64', 'float64']:\n",
    "        numeric = False\n",
    "    if col[:8] == 'COUNTRY_':\n",
    "        compensation_df = compensation_df.rename(columns={col: col[8:]}) \n",
    "    if col[:8] == 'EDLEVEL_':\n",
    "        compensation_df = compensation_df.rename(columns={col: col[8:]})\n",
    "\n",
    "if(numeric):\n",
    "    print('All columns are numerical\\n')\n",
    "else:\n",
    "    print('One or more columns are non-numerical\\n')\n",
    "\n",
    "#show new columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(compensation_df.shape)\n",
    "compensation_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot w/out outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(x=compensation_df['COMPENSATION'], color='darkturquoise')\n",
    "plt.title('Distribution of Compensation')\n",
    "plt.xlabel('Compensation (USD)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "#plot corr matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(compensation_df.corr(), cmap='RdYlGn')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "compensation_corr = compensation_df.corr()['COMPENSATION'].abs().sort_values(ascending=False)\n",
    "compensation_corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compensation_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "X = compensation_df.drop(columns='COMPENSATION')\n",
    "Y = compensation_df['COMPENSATION']\n",
    "\n",
    "#split the data\n",
    "X_train_c, X_test_c, Y_train_c, Y_test_c = train_test_split(X, Y, test_size=.2)\n",
    "Y_train_log = np.log1p(Y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
    "# Best Params: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "# MAE: 24753.358779181777\n",
    "# R^2: 0.5426305940721701\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Set up parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use log-transformed Y for training\n",
    "Y_train_log = np.log1p(Y_train_c)\n",
    "\n",
    "# Create model with best parameters from previous grid search\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# # Grid search with 3-fold CV\n",
    "# grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "# grid_search.fit(X_train_c, Y_train_log)\n",
    "# best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Fit directly using best parameters\n",
    "rf.fit(X_train_c, Y_train_log)\n",
    "\n",
    "# Predict and reverse log\n",
    "Y_pred_log = rf.predict(X_test_c)\n",
    "Y_pred_c = np.expm1(Y_pred_log)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "print(\"Best Params: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\")\n",
    "print(\"MAE:\", mean_absolute_error(Y_test_c, Y_pred_c))\n",
    "print(\"R^2:\", r2_score(Y_test_c, Y_pred_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_test_c - Y_pred_c\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Y_pred_c, residuals, alpha=0.4)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted Salary\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.title(\"Residual Plot Random Forest\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform the target variable\n",
    "Y_train_log = np.log1p(Y_train_c)\n",
    "\n",
    "# Create the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Cross-validate using negative MAE (optional, just for sanity)\n",
    "cv_scores = cross_val_score(lr, X_train_c, Y_train_log, cv=3, scoring='neg_mean_absolute_error')\n",
    "print(\"CV MAE scores (neg):\", cv_scores)\n",
    "print(\"Average MAE:\", -np.mean(cv_scores))\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_train_c, Y_train_log)\n",
    "\n",
    "# Predict and reverse log\n",
    "Y_pred_log = lr.predict(X_test_c)\n",
    "Y_pred_c = np.expm1(Y_pred_log)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MAE:\", mean_absolute_error(Y_test_c, Y_pred_c))\n",
    "print(\"R^2:\", r2_score(Y_test_c, Y_pred_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y_test_c - Y_pred_c\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Y_pred_c, residuals, alpha=0.4)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted Salary\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.title(\"Residual Plot Linear Regression\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------                       \n",
    "\n",
    ".\n",
    "\n",
    "**Job Satisfaction Data Exploration**\n",
    "\n",
    ".\n",
    "\n",
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all columns\n",
    "all_c_jobsat_df = odf_2024.copy()\n",
    "\n",
    "#drop currency, totalcomp, AITOOL, AINEXT, TBRANCH, SURVERYLENGTH, SURVEYEASE, RESPONSEID\n",
    "jobsat_onehot = [\n",
    "   'MAINBRANCH', 'REMOTEWORK', 'DEVTYPE', 'COUNTRY']\n",
    "\n",
    "jobsat_multilabel = [\n",
    "    'EMPLOYMENT', 'CODINGACTIVITIES', 'LEARNCODE', 'LEARNCODEONLINE', 'TECHDOC', 'BUYNEWTOOL',\n",
    "    'TECHENDORSE', 'NEWSOSITES', 'SOHOW', 'AIBEN', 'AIETHICS', 'AICHALLENGES', 'FRUSTRATION', 'PROFESSIONALTECH']\n",
    "\n",
    "jobsat_include_columns = [\n",
    "    'AGE', 'YEARSCODE', 'YEARSCODEPRO', 'WORKEXP', 'EDLEVEL', 'ORGSIZE', \n",
    "    'PURCHASEINFLUENCE', 'BUILDVSBUY', 'SOVISITFREQ', 'SOACCOUNT', 'SOPARTFREQ', 'SOCOMM',\n",
    "    'AISELECT', 'AISENT', 'AIACC', 'AICOMPLEX', 'AITHREAT', 'ICORPM', 'TIMESEARCHING', \"TIMEANSWERING\",\n",
    "    'PROFESSIONALCLOUD', 'JOBSAT', 'JOBSATPOINTS_1', 'JOBSATPOINTS_4', 'JOBSATPOINTS_5', 'JOBSATPOINTS_6', \n",
    "    'JOBSATPOINTS_7',  'JOBSATPOINTS_8',  'JOBSATPOINTS_9', 'JOBSATPOINTS_10', 'JOBSATPOINTS_11', 'KNOWLEDGE_1', \n",
    "    'KNOWLEDGE_2', 'KNOWLEDGE_3', 'KNOWLEDGE_4', 'KNOWLEDGE_5', 'KNOWLEDGE_6', 'KNOWLEDGE_7', 'KNOWLEDGE_8', \n",
    "    'KNOWLEDGE_9', 'FREQUENCY_1', 'FREQUENCY_2', 'FREQUENCY_3']\n",
    "\n",
    "#create jobsat\n",
    "useable_columns = jobsat_multilabel + jobsat_onehot + jobsat_include_columns\n",
    "jobsat_df = odf_2024[useable_columns]\n",
    "\n",
    "#drop all NaN\n",
    "\n",
    "print(f'Shape before drop: {jobsat_df.shape}')\n",
    "jobsat_df = jobsat_df.dropna()\n",
    "jobsat_df.reset_index(drop=True, inplace=True)\n",
    "print(f'Shape before drop: {jobsat_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "#apply ordinal encoding\n",
    "ordinal_maps = {\n",
    "    \"AGE\" : {\n",
    "        \"Under 18 years old\": 0, \"18-24 years old\": 1, \"25-34 years old\": 2,\n",
    "        \"35-44 years old\": 3, \"45-54 years old\": 4, \"55-64 years old\": 5,\n",
    "        \"65 years or older\": 6, \"Prefer not to say\": -1},\n",
    "    \"EDLEVEL\": {\n",
    "        \"Primary/elementary school\": 0, \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\": 1,\n",
    "        \"Some college/university study without earning a degree\": 2, \"Associate degree (A.A., A.S., etc.)\": 3,\n",
    "        \"Bachelorâ€™s degree (B.A., B.S., B.Eng., etc.)\": 4, \"Masterâ€™s degree (M.A., M.S., M.Eng., MBA, etc.)\": 5,\n",
    "        \"Professional degree (JD, MD, Ph.D, Ed.D, etc.)\": 6, \"Something else\": 7},\n",
    "    \"ORGSIZE\": {\n",
    "        \"Just me - I am a freelancer, sole proprietor, etc.\": 0, \"2 to 9 employees\": 1,\n",
    "        \"10 to 19 employees\": 2, \"20 to 99 employees\": 3, \"100 to 499 employees\": 4,\n",
    "        \"500 to 999 employees\": 5, \"1,000 to 4,999 employees\": 6, \"5,000 to 9,999 employees\": 7,\n",
    "        \"10,000 or more employees\": 8, \"I donâ€™t know\": -1},\n",
    "    \"PURCHASEINFLUENCE\": {\n",
    "        \"I have little or no influence\": 0, \"I have some influence\": 1, \n",
    "        \"I have a great deal of influence\": 2},\n",
    "    \"BUILDVSBUY\": {\n",
    "        \"Out-of-the-box is ready to go with little need for customization\": 0,\n",
    "        \"Is ready-to-go but also customizable for growth and targeted use cases\": 1,\n",
    "        \"Is set up to be customized and needs to be engineered into a usable product\": 2},\n",
    "    \"SOVISITFREQ\": {\n",
    "        \"Less than once per month or monthly\": 0, \"A few times per month or weekly\": 1,\n",
    "        \"A few times per week\": 2, \"Daily or almost daily\": 3, \"Multiple times per day\": 4},\n",
    "    \"SOPARTFREQ\": {\n",
    "        \"I have never participated in Q&A on Stack Overflow\": 0, \"Less than once per month or monthly\": 1,\n",
    "        \"A few times per month or weekly\": 2, \"A few times per week\": 3, \"Daily or almost daily\": 4,\n",
    "        \"Multiple times per day\": 5},\n",
    "    \"SOCOMM\": {\n",
    "        \"No, not at all\": 0, \"No, not really\": 1, \"Neutral\": 2,\n",
    "        \"Yes, somewhat\": 3, \"Yes, definitely\": 4, \"Not sure\": 5},\n",
    "    \"SOACCOUNT\": {\n",
    "        \"Yes\": 1, \"No\": 0},\n",
    "    \"AISENT\": {\n",
    "        \"Very unfavorable\": 0, \"Unfavorable\": 1, \"Indifferent\": 2,\n",
    "        \"Favorable\": 3, \"Very favorable\": 4, \"Unsure\": 5},\n",
    "    \"AIACC\": {\n",
    "        \"Highly distrust\": 0, \"Somewhat distrust\": 1, \"Neither trust nor distrust\": 2,\n",
    "        \"Somewhat trust\": 3, \"Highly trust\": 4},\n",
    "    \"AISELECT\" : {\n",
    "        \"Yes\": 1, \"No\": 0},\n",
    "    \"AICOMPLEX\": {\n",
    "        \"Very poor at handling complex tasks\": 0, \"Bad at handling complex tasks\": 1,\n",
    "        \"Neither good or bad at handling complex tasks\": 2, \"Good, but not great at handling complex tasks\": 3,\n",
    "        \"Very well at handling complex tasks\": 4},\n",
    "    \"AITHREAT\": {\n",
    "        \"No\": 0, \"I'm not sure\": 1, \"Yes\": 2},\n",
    "    \"ICORPM\": {\n",
    "        \"Individual contributor\": 0, \"People manager\": 1},\n",
    "    \"TIMESEARCHING\": {\n",
    "        \"Less than 15 minutes a day\": 0, \"15-30 minutes a day\": 1,\n",
    "        \"30-60 minutes a day\": 2, \"60-120 minutes a day\": 3, \"Over 120 minutes a day\": 4},\n",
    "    \"TIMEANSWERING\": {\n",
    "        \"Less than 15 minutes a day\": 0, \"15-30 minutes a day\": 1, \"30-60 minutes a day\": 2,\n",
    "        \"60-120 minutes a day\": 3, \"Over 120 minutes a day\": 4},\n",
    "    \"PROFESSIONALCLOUD\": {\n",
    "        \"On-prem\": 0, \"Hybrid (on-prem and cloud)\": 1, \"Cloud only (single or multi-cloud)\": 2}\n",
    "}\n",
    "for col, mapping in ordinal_maps.items():\n",
    "    if col in jobsat_df.columns:\n",
    "        jobsat_df[col] = jobsat_df[col].map(mapping).astype(int)\n",
    "\n",
    "knowledge_order = {\n",
    "    'Strongly agree': 4, \n",
    "    'Agree': 3,\n",
    "    'Neither agree nor disagree': 2,\n",
    "    'Disagree': 1,\n",
    "    'Strongly disagree': 0\n",
    "}\n",
    "for i in range(1, 10): \n",
    "    jobsat_df['KNOWLEDGE_'+str(i)] = jobsat_df['KNOWLEDGE_'+str(i)].map(knowledge_order)\n",
    "\n",
    "frequency_order = {\n",
    "    'Never': 0, \n",
    "    '1-2 times a week': 1,\n",
    "    '3-5 times a week': 2, \n",
    "    '6-10 times a week': 3, \n",
    "    '10+ times a week': 4\n",
    "}\n",
    "for i in range(1,4): \n",
    "    jobsat_df['FREQUENCY_'+str(i)] = jobsat_df['FREQUENCY_'+str(i)].map(frequency_order)\n",
    "\n",
    "\n",
    "#encode onehot cols\n",
    "jobsat_df = pd.get_dummies(jobsat_df, columns=jobsat_onehot, drop_first=True)\n",
    "\n",
    "#encode multilabel cols\n",
    "for col in jobsat_multilabel:\n",
    "    #split cols\n",
    "    split_col = jobsat_df[col].str.split(';')\n",
    "\n",
    "    #multilabel instance\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    multilabel_encoded = pd.DataFrame(\n",
    "        mlb.fit_transform(split_col),\n",
    "        columns=[f\"{col}_{cls}\" for cls in mlb.classes_],\n",
    "        index=jobsat_df.index\n",
    "    )\n",
    "\n",
    "    #add col to jobsat\n",
    "    jobsat_df = pd.concat([jobsat_df.drop(columns=col), multilabel_encoded], axis=1)\n",
    "\n",
    "#convert YEARSCODE to int\n",
    "jobsat_df['YEARSCODE'] = jobsat_df['YEARSCODE'].replace({\n",
    "    'More than 50 years': '50',\n",
    "    'Less than 1 year': '1'\n",
    "})\n",
    "jobsat_df['YEARSCODE'] = jobsat_df['YEARSCODE'].astype(int)\n",
    "\n",
    "#convert YEARSCODEPRO to int\n",
    "jobsat_df['YEARSCODEPRO'] = jobsat_df['YEARSCODEPRO'].replace({\n",
    "    'More than 50 years': '50',\n",
    "    'Less than 1 year': '1'\n",
    "})\n",
    "jobsat_df['YEARSCODEPRO'] = jobsat_df['YEARSCODEPRO'].astype(int)\n",
    "\n",
    "#convert everything else to int\n",
    "jobsat_df = jobsat_df.astype(int)\n",
    "\n",
    "#check all columns are ints\n",
    "all_ints = True\n",
    "for c in jobsat_df.columns:\n",
    "    if jobsat_df[c].dtype != np.int64:\n",
    "        all_ints = False\n",
    "        break\n",
    "\n",
    "if all_ints: print('All columns successfully converted to type int\\n')\n",
    "else: print('Error: One or more columns not of type int\\n')\n",
    "\n",
    "print(f'Shape after encoding: {jobsat_df.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsat_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reclass JOBSAT to low satisfaction, medium satisfaction, and high satisfaction\n",
    "def bin_jobsat(val):\n",
    "    if val <= 4:\n",
    "        return 0  #low satisfaction\n",
    "    elif val <= 7:\n",
    "        return 1  #medium\n",
    "    else:\n",
    "        return 2  #high\n",
    "\n",
    "jobsat_df[\"JOBSAT_BINNED\"] = jobsat_df[\"JOBSAT\"].apply(bin_jobsat)\n",
    "jobsat_df.drop(columns=\"JOBSAT\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = jobsat_df.drop(columns=[\"JOBSAT_BINNED\"])\n",
    "y = jobsat_df[\"JOBSAT_BINNED\"]\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#select the top 75 features combination\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=75)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "#get the names of the selected features\n",
    "selected_features = X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reassign X with seelcted features\n",
    "X = jobsat_df[selected_features]\n",
    "\n",
    "#split data into train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def run_fp_growth(X, y_binned, min_support_vals=[0.01, 0.03, 0.05]):\n",
    "\n",
    "    all_rules = []\n",
    "    for label in sorted(y_binned.unique()):\n",
    "        # Subset data\n",
    "        X_bin = X[y_binned == label]\n",
    "\n",
    "        # Only keep true (1) values in transactions\n",
    "        transactions = X_bin.apply(\n",
    "            lambda row: [f\"{col}={row[col]}\" for col in X_bin.columns if row[col] == 1],\n",
    "            axis=1\n",
    "        ).tolist()\n",
    "\n",
    "        # One-hot encode\n",
    "        te = TransactionEncoder()\n",
    "        te_ary = te.fit(transactions).transform(transactions)\n",
    "        df_trans = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "        for sup in min_support_vals:\n",
    "            fi = fpgrowth(df_trans, min_support=sup, use_colnames=True)\n",
    "            if fi.empty or fi['itemsets'].apply(len).max() < 2:\n",
    "                continue\n",
    "\n",
    "            rules = association_rules(fi, metric=\"lift\", min_threshold=0.7)\n",
    "            if rules.empty:\n",
    "                continue\n",
    "\n",
    "            for _, r in rules.iterrows():\n",
    "                all_rules.append({\n",
    "                    'bin': label,\n",
    "                    'support': r['support'],\n",
    "                    'confidence': r['confidence'],\n",
    "                    'lift': r['lift'],\n",
    "                    'antecedents': ', '.join(sorted(r['antecedents'])),\n",
    "                    'consequents': ', '.join(sorted(r['consequents'])),\n",
    "                    'min_support': sup,\n",
    "                })\n",
    "\n",
    "    if all_rules:\n",
    "        rules_df = pd.DataFrame(all_rules)\n",
    "        rules_df = rules_df.sort_values(['bin', 'lift'], ascending=[True, False])\n",
    "        print(\"\\n=== Association Rules Summary ===\")\n",
    "        print(rules_df.to_string(index=False))\n",
    "        return rules_df\n",
    "    else:\n",
    "        print(\"No association rules found for the given parameters.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "fp_results = run_fp_growth(X_train, y_train, [0.1, 0.3, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes fp_results is a DataFrame with columns: 'bin', 'min_support', 'support', 'confidence', 'lift', etc.\n",
    "\n",
    "for support_val in sorted(fp_results['min_support'].unique()):\n",
    "    subset = fp_results[fp_results['min_support'] == support_val]\n",
    "    \n",
    "    avg_support = subset['support'].mean()\n",
    "    avg_confidence = subset['confidence'].mean()\n",
    "    avg_lift = subset['lift'].mean()\n",
    "\n",
    "    print(f\"\\nMetrics for min_support = {support_val}\")\n",
    "    print(f\"Average Support:    {avg_support:.4f}\")\n",
    "    print(f\"Average Confidence: {avg_confidence:.4f}\")\n",
    "    print(f\"Average Lift:       {avg_lift:.4f}\")\n",
    "    print(f\"Rule Count:         {len(subset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fp_results['support'].hist(bins=20)\n",
    "plt.title(\"Support Distribution\")\n",
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "fp_results['lift'].hist(bins=20)\n",
    "plt.title(\"Lift Distribution\")\n",
    "plt.xlabel(\"Lift\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "K_values = range(2, 11) \n",
    "\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_values, inertias, marker='o')\n",
    "plt.title(\"Elbow Method: Inertia vs. Number of Clusters (k)\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "db_scores = []\n",
    "\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    sil_score = silhouette_score(X_scaled, cluster_labels)\n",
    "    db_score = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "    \n",
    "    silhouette_scores.append(sil_score)\n",
    "    db_scores.append(db_score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_values, silhouette_scores, marker='o', color='blue', label='Silhouette')\n",
    "plt.title(\"Silhouette Score vs. k\")\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score (higher=better)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Davies-Bouldin Index\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K_values, db_scores, marker='o', color='red', label='Davies-Bouldin')\n",
    "plt.title(\"Davies-Bouldin Index vs. k\")\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Davies-Bouldin Index (lower=better)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_k = 2\n",
    "kmeans_final = KMeans(n_clusters=final_k, n_init=10, random_state=42)\n",
    "final_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "jobsat_clusters = jobsat_df.copy()\n",
    "jobsat_clusters[\"Cluster\"] = final_labels\n",
    "print(jobsat_clusters.head(5))\n",
    "\n",
    "# Evaluate final model\n",
    "final_sil_score = silhouette_score(X_scaled, final_labels)\n",
    "final_db_score = davies_bouldin_score(X_scaled, final_labels)\n",
    "\n",
    "print(f\"Final K-Means results (k={final_k}):\")\n",
    "print(f\"Silhouette Score  = {final_sil_score:.3f} (higher is better)\")\n",
    "print(f\"Davies-Bouldin    = {final_db_score:.3f} (lower is better)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = jobsat_clusters[\"Cluster\"].value_counts().sort_index()\n",
    "print(\"Number of rows in each cluster:\")\n",
    "print(cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Cluster' and compute means of all features\n",
    "cluster_summary = jobsat_clusters.groupby(\"Cluster\").mean()\n",
    "\n",
    "print(\"Average feature values per cluster:\")\n",
    "display(cluster_summary.head())\n",
    "\n",
    "cluster_summary_median = jobsat_clusters.groupby(\"Cluster\").median()\n",
    "print(\"Median feature values per cluster:\")\n",
    "display(cluster_summary_median.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance across clusters for each feature\n",
    "\n",
    "variances = cluster_summary.var(axis=0)\n",
    "\n",
    "# Sort features by descending variance\n",
    "top_10_features = variances.nlargest(10).index\n",
    "print(\"Top 10 features that differ the most across clusters:\")\n",
    "print(top_10_features)\n",
    "\n",
    "# Look at the mean values of just those top 10\n",
    "summary_top10 = cluster_summary[top_10_features]\n",
    "print(\"\\nMean values of top 10 differing features by cluster:\")\n",
    "display(summary_top10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(summary_top10, annot=True, cmap=\"viridis\")\n",
    "plt.title(\"Mean Values of Top 10 Differing Features by Cluster\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cross_tab = pd.crosstab(jobsat_clusters['Cluster'], jobsat_clusters['JOBSAT_BINNED'])\n",
    "print(\"Cluster vs. Job Satisfaction Bins:\")\n",
    "print(cross_tab)\n",
    "\n",
    "cross_tab.plot(kind='bar', stacked=True, figsize=(8,6), colormap='Spectral')\n",
    "plt.title(\"Job Satisfaction Bins within Each Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"JOBSAT_BINNED\", loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=final_k, n_init=10, random_state=42)\n",
    "cluster_labels = kmeans_model.fit_predict(X_scaled)\n",
    "\n",
    "# PCA for 2D Visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    X_pca[:, 0], \n",
    "    X_pca[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='Accent', \n",
    "    s=60, alpha=0.5\n",
    ")\n",
    "plt.title(f\"Final Cluster Assignments (k={final_k}) in 2D PCA Projection\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
